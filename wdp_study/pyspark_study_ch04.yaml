---
title: "PySpark Study - Chapter 4 Prepare Data for Modeling"
accessControlType: "SpacePublicReadOnly"
sparkInterpreterTypeId: 4
disableRun: false
embeddedParameters: []
notebookSparkSubmitOptions: []
paragraphs:
- title: "Duplicates"
  width: "LG"
  code: |+
    %pyspark
    import pyspark.sql.functions as fn

    df = spark.createDataFrame([
        (1, 144.5, 5.9, 33, 'M'),
        (2, 167.2, 5.4, 45, 'M'),
        (3, 124.1, 5.2, 23, 'F'),
        (4, 144.5, 5.9, 33, 'M'),
        (5, 133.2, 5.7, 54, 'F'),
        (3, 124.1, 5.2, 23, 'F'),
        (5, 129.2, 5.3, 42, 'M'), ],
        ['id', 'weight', 'height','age', 'gender'])

    ## Duplicates
    print("[Duplicates] Count of rows: df.count() => {0}".format(df.count()))
    print("[Duplicates] Count of distinct rows: df.distinct().count() => {0}".format(df.distinct().count()))

    df = df.dropDuplicates()
    print("[Duplicates] df.dropDuplicates().show() =>")
    df.show()

    print("[Duplicates] Count of ids: df.count() => {0}".format(df.count()))
    print("[Duplicates] Count of distinct ids:  df.select([c for c in df.columns if c != 'id']).distinct().count() => {0}".format(
        df.select([
            c for c in df.columns if c != 'id'
        ]).distinct().count())
    )


    df = df.dropDuplicates(subset=[
        c for c in df.columns if c != 'id'
    ])
    print("[Duplicates] df.dropDuplicates(subset=[c for c in df.columns if c != 'id']) => ")
    df.show()


    print("[Duplicates] df.agg(fn.count('id').alias('count'), fn.countDistinct('id').alias('distinct')) => ")
    df.agg(
        fn.count('id').alias('count'), fn.countDistinct('id').alias('distinct')
    ).show()


    print("[Duplicates] df.withColumn('new_id', fn.monotonically_increasing_id()) => ")
    df.withColumn('new_id', fn.monotonically_increasing_id()).show()


  executionResultDisplayFormat: "Text"
  barchartX: null
  barchartY: null
  barchartAggregation: null
  barchartGroup: null
  barchartStacked: false
  linechartX: null
  linechartXType: null
  linechartXFormat: null
  linechartY: null
  linechartAggregation: null
  linechartGroup: null
  tableHeight: 306
  barchartHeight: 306
  linechartHeight: 306
  hidden: false
  linechartYMin: null
  linechartYMax: null
  disableRun: false
  pivotHeight: 306
  pivotAggregationIndex: null
  pivotReduceMethod: null
  pivotAvailableFields: null
  pivotColFields: null
  pivotRowFields: null
- title: "Missing observations"
  width: "LG"
  code: |-
    %pyspark
    import pyspark.sql.functions as fn

    ## Missing observations
    df_miss = spark.createDataFrame([
        (1, 143.5, 5.6, 28, 'M', 100000),
        (2, 167.2, 5.4, 45, 'M', None),
        (3, None , 5.2, None, None, None),
        (4, 144.5, 5.9, 33, 'M', None),
        (5, 133.2, 5.7, 54, 'F', None),
        (6, 124.1, 5.2, None, 'F', None),
        (7, 129.2, 5.3, 42, 'M', 76000),],
        ['id', 'weight', 'height', 'age', 'gender', 'income'])


    print("[Missing observations] df_miss.rdd.map( \n    lambda row: (row['id'], sum([c == None for c in row]))\n).collect() => \n {0}".format(df_miss.rdd.map(lambda row: (row['id'], sum([c == None for c in row]))).collect()))
    df_miss.rdd.map(
        lambda row: (row['id'], sum([c == None for c in row]))
    ).collect()

    print("[Missing observations] df_miss.where('id == 3').show() =>")
    df_miss.where('id == 3').show()


    print("[Missing observations] df_miss.agg(*[ \n (1 - (fn.count(c) / fn.count('*'))).alias(c + '_missing') for c in df_miss.columns \n]).show() =>")
    df_miss.agg(*[
        (1 - (fn.count(c) / fn.count('*'))).alias(c + '_missing') for c in df_miss.columns
    ]).show()



    df_miss_no_income = df_miss.select([
        c for c in df_miss.columns if c != 'income'
    ])

    print("[Missing observations] df_miss_no_income.dropna(thresh=3).show() => ")
    df_miss_no_income.dropna(thresh=3).show()


    # fillna
    means = df_miss_no_income.agg(
        *[fn.mean(c).alias(c)
            for c in df_miss_no_income.columns if c != 'gender']
    ).toPandas().to_dict('records')[0]

    means['gender'] = 'missing'

    print("[Missing observations] df_miss_no_income.fillna(means).show() => ")
    df_miss_no_income.fillna(means).show()
  executionResultDisplayFormat: null
  barchartX: null
  barchartY: null
  barchartAggregation: null
  barchartGroup: null
  barchartStacked: false
  linechartX: null
  linechartXType: null
  linechartXFormat: null
  linechartY: null
  linechartAggregation: null
  linechartGroup: null
  tableHeight: 306
  barchartHeight: 306
  linechartHeight: 306
  hidden: false
  linechartYMin: null
  linechartYMax: null
  disableRun: false
  pivotHeight: 306
  pivotAggregationIndex: null
  pivotReduceMethod: null
  pivotAvailableFields: null
  pivotColFields: null
  pivotRowFields: null
- title: "Outliers"
  width: "LG"
  code: |
    %pyspark

    df_outliers = spark.createDataFrame([
        (1, 143.5, 5.3, 28),
        (2, 154.2, 5.5, 45),
        (3, 342.3, 5.1, 99),
        (4, 144.5, 5.5, 33),
        (5, 133.2, 5.4, 54),
        (6, 124.1, 5.1, 21),
        (7, 129.2, 5.3, 42),],
        ['id', 'weight', 'height', 'age'])

    cols = ['weight', 'height', 'age']
    bounds = {}

    for col in cols:
        quantiles = df_outliers.approxQuantile(
            col, [0.25, 0.75], 0.05
        )

        IQR = quantiles[1] - quantiles[0]

        bounds[col] = [
            quantiles[0] - 1.5 * IQR,
            quantiles[1] + 1.5 * IQR
        ]

    print("[Outliers] bounds : {0}".format(bounds))


    outliers = df_outliers.select(*['id'] + [
        (
            (df_outliers[c] < bounds[c][0]) |
            (df_outliers[c] > bounds[c][1])
        ).alias(c + '_o') for c in cols
    ])
    print("[Outliers] outliers => ")
    outliers.show()


    df_outliers = df_outliers.join(outliers, on='id')

    print("[Outliers] df_outliers.filter('weight_o').select('id', 'weight').show()  => ")
    df_outliers.filter('weight_o').select('id', 'weight').show()

    print("[Outliers] df_outliers.filter('age_o').select('id', 'age').show()  => ")
    df_outliers.filter('age_o').select('id', 'age').show()
  executionResultDisplayFormat: "Text"
  barchartX: null
  barchartY: null
  barchartAggregation: null
  barchartGroup: null
  barchartStacked: false
  linechartX: null
  linechartXType: null
  linechartXFormat: null
  linechartY: null
  linechartAggregation: null
  linechartGroup: null
  tableHeight: 306
  barchartHeight: 306
  linechartHeight: 306
  hidden: false
  linechartYMin: null
  linechartYMax: null
  disableRun: false
  pivotHeight: 306
  pivotAggregationIndex: null
  pivotReduceMethod: null
  pivotAvailableFields: null
  pivotColFields: null
  pivotRowFields: null
- title: "Getting familiar with your data"
  width: "LG"
  code: |+
    %pyspark
    import pyspark.sql.types as typ

    fraud = sc.textFile("/user/fp10300/wbp_study/chapter4.csv.gz")
    header = fraud.first()

    fraud = fraud \
        .filter(lambda row: row != header) \
        .map(lambda row: [int(elem) for elem in row.split(',')])

    fields = [
        *[
            typ.StructField(h[1:-1], typ.IntegerType(), True)
            for h in header.split(',')
        ]
    ]
    schema = typ.StructType(fields)

    fraud_df = spark.createDataFrame(fraud, schema)


    ## Descriptive statistics
    print("[Descriptive statistics] fraud_df.printSchema()  => ")
    fraud_df.printSchema()

    print("[Descriptive statistics] fraud_df.groupby('gender').count().show() => ")
    fraud_df.groupby('gender').count().show()


    numerical = ['balance', 'numTrans', 'numIntlTrans']
    desc = fraud_df.describe(numerical)

    print("[Descriptive statistics] fraud_df.describe(numerical) => ")
    desc.show()

    print("[Descriptive statistics] fraud_df.agg({'balance': 'skewness'}).show() => ")
    fraud_df.agg({'balance': 'skewness'}).show()


    ## Correlations
    print("[Correlations] fraud_df.corr('balance', 'numTrans') => {0}".format(fraud_df.corr('balance', 'numTrans')))


    n_numerical = len(numerical)

    corr = []

    for i in range(0, n_numerical):
        temp = [None] * i

        for j in range(i, n_numerical):
            temp.append(fraud_df.corr(numerical[i], numerical[j]))
        corr.append(temp)



  executionResultDisplayFormat: "Text"
  barchartX: null
  barchartY: null
  barchartAggregation: null
  barchartGroup: null
  barchartStacked: false
  linechartX: null
  linechartXType: null
  linechartXFormat: null
  linechartY: null
  linechartAggregation: null
  linechartGroup: null
  tableHeight: 306
  barchartHeight: 306
  linechartHeight: 306
  hidden: false
  linechartYMin: null
  linechartYMax: null
  disableRun: false
  pivotHeight: 306
  pivotAggregationIndex: null
  pivotReduceMethod: null
  pivotAvailableFields: null
  pivotColFields: null
  pivotRowFields: null
- title: "Visualization - histogram"
  width: "LG"
  code: |+
    %pyspark
    import matplotlib
    import matplotlib.dates as mdates
    import matplotlib.pyplot as plt
    import pyspark.sql.types as typ
    import pandas as pd
    import sys
    import bokeh

    # fraud = sc.textFile("/user/fp10300/wbp_study/chapter4.csv.gz")
    # header = fraud.first()

    # fraud = fraud \
    #     .filter(lambda row: row != header) \
    #     .map(lambda row: [int(elem) for elem in row.split(',')])

    # fields = [
    #     *[
    #         typ.StructField(h[1:-1], typ.IntegerType(), True)
    #         for h in header.split(',')
    #     ]
    # ]
    # schema = typ.StructType(fields)

    # fraud_df = spark.createDataFrame(fraud, schema)

    hists = fraud_df.select('balance').rdd.flatMap(
            lambda row: row
        ).histogram(20)


    ## Histograms
    data = {
        'bins': hists[0][:-1],
        'freq': hists[1]
    }
    plt.bar(data['bins'], data['freq'], width=2000)
    plt.title('Histogram of \'balance\'')
    plt.savefig(sys.stdout, format='svg')

  executionResultDisplayFormat: "Text"
  barchartX: null
  barchartY: null
  barchartAggregation: null
  barchartGroup: null
  barchartStacked: false
  linechartX: null
  linechartXType: null
  linechartXFormat: null
  linechartY: null
  linechartAggregation: null
  linechartGroup: null
  tableHeight: 306
  barchartHeight: 306
  linechartHeight: 306
  hidden: false
  linechartYMin: null
  linechartYMax: null
  disableRun: false
  pivotHeight: 306
  pivotAggregationIndex: null
  pivotReduceMethod: null
  pivotAvailableFields: null
  pivotColFields: null
  pivotRowFields: null
- title: "Visuliazaion - line chart"
  width: "LG"
  code: |-
    %pyspark
    import matplotlib
    import matplotlib.dates as mdates
    import matplotlib.pyplot as plt
    import pandas as pd
    import sys
    df = spark.sql("select yyyymmdd, rand() as rand from default.time_master where yyyymm between '201801' and '201812'").toPandas()
    df['yyyymmdd'] = pd.to_datetime(df['yyyymmdd'])
    df.plot(x='yyyymmdd', figsize=(16, 4))
    plt.savefig(sys.stdout, format='svg')
  executionResultDisplayFormat: "Text"
  barchartX: null
  barchartY: null
  barchartAggregation: null
  barchartGroup: null
  barchartStacked: false
  linechartX: null
  linechartXType: null
  linechartXFormat: null
  linechartY: null
  linechartAggregation: null
  linechartGroup: null
  tableHeight: 306
  barchartHeight: 306
  linechartHeight: 306
  hidden: false
  linechartYMin: null
  linechartYMax: null
  disableRun: false
  pivotHeight: 306
  pivotAggregationIndex: null
  pivotReduceMethod: null
  pivotAvailableFields: null
  pivotColFields: null
  pivotRowFields: null
